{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activations:\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        a = np.maximum(0,z)\n",
    "        return a\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        return np.tanh(z) \n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(dA, z):\n",
    "        sig = sigmoid(z)\n",
    "        return dA * sig * (1 - sig)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(self,z):\n",
    "            return 0 if z < 0 else 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(z):\n",
    "       return 1-np.tanh(z)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,layer_sizes, epochs, alpha):\n",
    "        \n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_iters = epochs\n",
    "        self.learning_rate = alpha\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ANN Hyperparameters\\nLayers: {self.layers_sizes}\\nLearning rate: {self.learning_rate} \\\n",
    "        \\nIterations: {self.num_iters}\"\n",
    "    \n",
    "    def initialize_params(self):\n",
    "        params = {}\n",
    "        for i in range(1, len(self.layer_sizes)):\n",
    "            params['W' + str(i)] = np.random.randn(self.layer_sizes[i], self.layer_sizes[i-1])*0.01\n",
    "            params['B' + str(i)] = np.random.randn(self.layer_sizes[i],1)*0.01\n",
    "        return params\n",
    "\n",
    "    def forward_propagation(self, X_train, params):\n",
    "        layers = len(params)//2\n",
    "        values = {}\n",
    "        for i in range(1, layers+1):\n",
    "            if i==1:\n",
    "                values['Z' + str(i)] = np.dot(params['W' + str(i)], X_train) + params['B' + str(i)]\n",
    "                values['A' + str(i)] = Activations.tanh(values['Z' + str(i)])\n",
    "            else:\n",
    "                values['Z' + str(i)] = np.dot(params['W' + str(i)], values['A' + str(i-1)]) + params['B' + str(i)]\n",
    "                if i==layers:\n",
    "                    values['A' + str(i)] = values['Z' + str(i)]\n",
    "                else:\n",
    "                    values['A' + str(i)] = Activations.relu(values['Z' + str(i)])\n",
    "        return values\n",
    "\n",
    "    def compute_cost(self, values, Y_train):\n",
    "        layers = len(values)//2\n",
    "        Y_pred = values['A' + str(layers)]\n",
    "        cost = 1/(2*len(Y_train)) * np.sum(np.square(Y_pred - Y_train))\n",
    "        return cost\n",
    "\n",
    "    def backward_propagation(self, params, values, X_train, Y_train):\n",
    "        layers = len(params)//2\n",
    "        m = len(Y_train)\n",
    "        grads = {}\n",
    "        for i in range(layers,0,-1):\n",
    "            if i==layers:\n",
    "                dA = 1/m * (values['A' + str(i)] - Y_train)\n",
    "                dZ = dA\n",
    "            else:\n",
    "                dA = np.dot(params['W' + str(i+1)].T, dZ)\n",
    "                dZ = np.multiply(dA, np.where(values['A' + str(i)]>=0, 1, 0))\n",
    "            if i==1:\n",
    "                grads['W' + str(i)] = 1/m * np.dot(dZ, X_train.T)\n",
    "                grads['B' + str(i)] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "            else:\n",
    "                grads['W' + str(i)] = 1/m * np.dot(dZ,values['A' + str(i-1)].T)\n",
    "                grads['B' + str(i)] = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        return grads\n",
    "\n",
    "    def update_params(self, params, grads):\n",
    "        layers = len(params)//2\n",
    "        params_updated = {}\n",
    "        for i in range(1,layers+1):\n",
    "            params_updated['W' + str(i)] = params['W' + str(i)] - self.learning_rate * grads['W' + str(i)]\n",
    "            params_updated['B' + str(i)] = params['B' + str(i)] - self.learning_rate * grads['B' + str(i)]\n",
    "        return params_updated\n",
    "\n",
    "    def train(self, X_train, Y_train):\n",
    "        params = self.initialize_params()\n",
    "        for i in range(self.num_iters):\n",
    "            values = self.forward_propagation(X_train.T, params)\n",
    "            cost = self.compute_cost(values, Y_train.T)\n",
    "            grads = self.backward_propagation(params, values,X_train.T, Y_train.T)\n",
    "            params = self.update_params(params, grads)\n",
    "            print('Cost at iteration ' + str(i+1) + ' = ' + str(cost) + '\\n')\n",
    "        return params\n",
    "    \n",
    "    def compute_accuracy(self, X_train, X_test, Y_train, Y_test, params):\n",
    "        values_train = self.forward_propagation(X_train.T, params)\n",
    "        values_test = self.forward_propagation(X_test.T, params)\n",
    "        train_acc = np.sqrt(mean_squared_error(Y_train, values_train['A' + str(len(self.layer_sizes)-1)].T))\n",
    "        test_acc = np.sqrt(mean_squared_error(Y_test, values_test['A' + str(len(self.layer_sizes)-1)].T))\n",
    "        return train_acc, test_acc\n",
    "\n",
    "    def predict(self, X, params):\n",
    "        values = self.forward_propagation(X.T, params)\n",
    "        predictions = values['A' + str(len(values)//2)].T\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "data = pd.read_csv(\"data_banknote_authentication.txt\", header=None)\n",
    "# split into input and output columns\n",
    "X, Y = data.values[:, :-1], data.values[:, -1] #separate data into input and output features\n",
    "# ensure all data are floating point values\n",
    "X = X.astype('float32')\n",
    "# encode strings to integer\n",
    "Y = LabelEncoder().fit_transform(Y)\n",
    "# split into train and test datasets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33) #split data into train and test sets in 80-20 ratio\n",
    "# determine the number of input features\n",
    "n_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eFKH6J_pUs65"
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork([4,5,5,1],10,0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at iteration 1 = 0.22469139202385582\n",
      "\n",
      "Cost at iteration 2 = 0.22468480189905315\n",
      "\n",
      "Cost at iteration 3 = 0.22467821220456924\n",
      "\n",
      "Cost at iteration 4 = 0.224671622940376\n",
      "\n",
      "Cost at iteration 5 = 0.2246650341064453\n",
      "\n",
      "Cost at iteration 6 = 0.22465844570274907\n",
      "\n",
      "Cost at iteration 7 = 0.22465185772925925\n",
      "\n",
      "Cost at iteration 8 = 0.22464527018594768\n",
      "\n",
      "Cost at iteration 9 = 0.22463868307278637\n",
      "\n",
      "Cost at iteration 10 = 0.2246320963897471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = nn.train(X_train,Y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nn.predict(X_train[4], params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, test_acc = nn.compute_accuracy(X_train, X_test, Y_train, Y_test, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Training Data = 0.670261904238637\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Training Data = ' + str(train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Data = 0.6568719978238466\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Test Data = ' + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions =  [[0.00137821]\n",
      " [0.00137841]\n",
      " [0.00137421]\n",
      " [0.00137949]\n",
      " [0.00137405]]\n"
     ]
    }
   ],
   "source": [
    "print('Predictions = ', predictions)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "F21BC.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
