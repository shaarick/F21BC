{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CW practice 3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP/e24yHDFD41BTR4uhKihh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaarick/F21BC/blob/main/Step%201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1RqBXtOq-H7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TLyMC9FQN5N"
      },
      "source": [
        "# list with parameters describing a neural network\n",
        "  # input dimension = input signal vector size for the layer\n",
        "  # output dimension = output activation vector of the layer\n",
        "  # activation function used on the layer\n",
        "nn_parameters = [\n",
        "    {\"input_dimension\": 2, \"output_dimension\": 4, \"activation\": \"tanh\"}, # tanh used on first layer\n",
        "    {\"input_dimension\": 4, \"output_dimension\": 6, \"activation\": \"relu\"}, # relu used on hidden layers\n",
        "    {\"input_dimension\": 6, \"output_dimension\": 6, \"activation\": \"relu\"},\n",
        "    {\"input_dimension\": 6, \"output_dimension\": 4, \"activation\": \"relu\"}, \n",
        "    {\"input_dimension\": 4, \"output_dimension\": 1, \"activation\": \"sigmoid\"}, # sigmoid used on final layer\n",
        "]\n"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk0BZUxfrVq5"
      },
      "source": [
        "# initiates the weights and biases of the nn\n",
        "def init_layers(nn_parameters, seed = 90):\n",
        "    np.random.seed(seed)\n",
        "    number_layers = len(nn_parameters)\n",
        "    param_values = {}\n",
        "\n",
        "    for index, layer in enumerate(nn_parameters):\n",
        "        layer_index = index + 1\n",
        "        input_size = layer[\"input_dimension\"]\n",
        "        output_size = layer[\"output_dimension\"]\n",
        "        \n",
        "        param_values['Weight' + str(layer_index)] = np.random.randn(\n",
        "            output_size, input_size) * 0.1\n",
        "        param_values['bias' + str(layer_index)] = np.random.randn(\n",
        "            output_size, 1) * 0.1\n",
        "        \n",
        "    return param_values"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSSJI4TYr7_q"
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x) \n",
        "\n",
        "def sigmoid_derivative(dA, x):\n",
        "    sig = sigmoid(x)\n",
        "    return dA * sig * (1 - sig)\n",
        "\n",
        "def relu_derivative(self,x):\n",
        "        return 0 if x < 0 else 1\n",
        "\n",
        "def tanh_derivative(x):\n",
        "   return 1-np.tanh(x)**2\n"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZK8j42nsg5B",
        "outputId": "8c599e92-5d3b-48a6-f3a1-69cf3d29f30c"
      },
      "source": [
        "sigmoid(7), relu(7), tanh(0.5), relu(-7)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9990889488055994, 7, 0.46211715726000974, 0)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dr_6Epgsc8x"
      },
      "source": [
        "# forward propagation of a single layer\n",
        "def forward_prop_single_layer(A_prev, Weight_curr, bias_curr, activation=\"relu\"):\n",
        "    x_curr = np.dot(Weight_curr, A_prev) + bias_curr\n",
        "    \n",
        "    if activation is \"tanh\":\n",
        "        activ_func = tanh\n",
        "    elif activation is \"relu\":\n",
        "        activ_func = relu\n",
        "    elif activation is \"sigmoid\":\n",
        "        activ_func = sigmoid\n",
        "    else:\n",
        "        raise Exception('Non-supported activation function')\n",
        "        \n",
        "    return activation_func(x_curr), x_curr"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOHSsGOdsmx-"
      },
      "source": [
        "def full_forward_prop(X, param_values, nn_parameters):\n",
        "    memory = {}\n",
        "    A_curr = X\n",
        "    \n",
        "    for index, layer in enumerate(nn_parameters):\n",
        "        layer_index = index + 1\n",
        "        A_prev = A_curr\n",
        "        \n",
        "        activ_function_curr = layer[\"activation\"]\n",
        "        Weight_curr = param_values[\"Weight\" + str(layer_index)]\n",
        "        bias_curr = param_values[\"bias\" + str(layer_index)]\n",
        "        A_curr, x_curr = forward_prop_single_layer(A_prev, Weight_curr, bias_curr, activ_function_curr)\n",
        "        \n",
        "        memory[\"A\" + str(index)] = A_prev\n",
        "        memory[\"x\" + str(layer_index)] = x_curr\n",
        "       \n",
        "    return A_curr, memory"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeW6zFwDspca"
      },
      "source": [
        "# cost function and accuracy value calculation\n",
        "def get_cost(Y_hat, Y):\n",
        "    m = Y_hat.shape[1]\n",
        "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T)) # binary crossentropy used as problem is binary classification\n",
        "    return np.squeeze(cost)\n",
        "\n",
        "def get_accuracy(Y_hat, Y):\n",
        "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
        "    return (Y_hat_ == Y).all(axis=0).mean()"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XivT6wb8srum"
      },
      "source": [
        "def backward_prop_single_layer(dA_curr, Weight_curr, bias_curr, x_curr, A_prev, activation=\"relu\"):\n",
        "    m = A_prev.shape[1]\n",
        "    \n",
        "    if activation is \"tanh\":\n",
        "        backward_activ_func = tanh_derivative\n",
        "    elif activation is \"relu\":\n",
        "        backward_activ_func = relu_derivative\n",
        "    elif activation is \"sigmoid\":\n",
        "        backward_activ_func = sigmoid_derivative\n",
        "    else:\n",
        "        raise Exception('Non-supported activation function')\n",
        "    \n",
        "    dx_curr = backward_activ_func(dA_curr, x_curr)\n",
        "    dWeight_curr = np.dot(dx_curr, A_prev.T) / m\n",
        "    dbias_curr = np.sum(dx_curr, axis=1, keepdims=True) / m\n",
        "    dA_prev = np.dot(Weight_curr.T, dx_curr)\n",
        "\n",
        "    return dA_prev, dWeight_curr, dbias_curr"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0DzUeSQsvJn"
      },
      "source": [
        "def full_backward_prop(Y_hat, Y, memory, param_values, nn_parameters):\n",
        "    gradient_values = {}\n",
        "    m = Y.shape[1]\n",
        "    Y = Y.reshape(Y_hat.shape)\n",
        "   \n",
        "    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
        "    \n",
        "    for layer_index_prev, layer in reversed(list(enumerate(nn_parameters))):\n",
        "        layer_index_curr = layer_index_prev + 1\n",
        "        activ_function_curr = layer[\"activation\"]\n",
        "        \n",
        "        dA_curr = dA_prev\n",
        "        \n",
        "        A_prev = memory[\"A\" + str(layer_index_prev)]\n",
        "        x_curr = memory[\"x\" + str(layer_index_curr)]\n",
        "        Weight_curr = param_values[\"Weight\" + str(layer_index_curr)]\n",
        "        bias_curr = param_values[\"bias\" + str(layer_index_curr)]\n",
        "        \n",
        "        dA_prev, dWeight_curr, dbias_curr = backward_prop_single_layer(\n",
        "            dA_curr, Weight_curr, bias_curr, x_curr, A_prev, activ_function_curr)\n",
        "        \n",
        "        gradient_values[\"dWeight\" + str(layer_index_curr)] = dWeight_curr\n",
        "        gradient_values[\"dbias\" + str(layer_index_curr)] = dbias_curr\n",
        "    \n",
        "    return gradient_values"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDiUUXpqsxj-"
      },
      "source": [
        "# using gradient descent to update parameter values\n",
        "def update(param_values, gradient_values, nn_parameters, learning_rate):\n",
        "    for layer_index, layer in enumerate(nn_parameters):\n",
        "        param_values[\"Weight\" + str(layer_index)] -= learning_rate * gradient_values[\"dWeight\" + str(layer_index)]        \n",
        "        param_values[\"bias\" + str(layer_index)] -= learning_rate * gradient_values[\"dbias\" + str(layer_index)]\n",
        "\n",
        "    return param_values;"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsDRv54dszmX"
      },
      "source": [
        "def train(X, Y, nn_parameters, epochs, learning_rate):\n",
        "    param_values = init_layers(nn_parameters, 2)\n",
        "    cost_history = []\n",
        "    accuracy_history = []\n",
        "    \n",
        "    for i in range(epochs):\n",
        "        Y_hat, cashe = full_forward_prop(X, param_values, nn_parameters)\n",
        "        cost = get_cost(Y_hat, Y)\n",
        "        cost_history.append(cost)\n",
        "        accuracy = get_accuracy(Y_hat, Y)\n",
        "        accuracy_history.append(accuracy)\n",
        "        \n",
        "        gradient_values = full_backward_prop(Y_hat, Y, cashe, param_values, nn_parameters)\n",
        "        param_values = update(param_values, gradient_values, nn_parameters, learning_rate)\n",
        "        \n",
        "    return param_values, cost_history, accuracy_history"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2iTT8zHBGRQ"
      },
      "source": [
        "df = pd.read_csv(\"data_banknote_authentication.txt\")\n"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_oFtI9vDBgvz",
        "outputId": "3f8f907b-f863-4604-b0de-b08e4f9098a0"
      },
      "source": [
        "df.head()\n"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>3.6216</th>\n",
              "      <th>8.6661</th>\n",
              "      <th>-2.8073</th>\n",
              "      <th>-0.44699</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.54590</td>\n",
              "      <td>8.1674</td>\n",
              "      <td>-2.4586</td>\n",
              "      <td>-1.46210</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3.86600</td>\n",
              "      <td>-2.6383</td>\n",
              "      <td>1.9242</td>\n",
              "      <td>0.10645</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.45660</td>\n",
              "      <td>9.5228</td>\n",
              "      <td>-4.0112</td>\n",
              "      <td>-3.59440</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.32924</td>\n",
              "      <td>-4.4552</td>\n",
              "      <td>4.5718</td>\n",
              "      <td>-0.98880</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.36840</td>\n",
              "      <td>9.6718</td>\n",
              "      <td>-3.9606</td>\n",
              "      <td>-3.16250</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    3.6216  8.6661  -2.8073  -0.44699  0\n",
              "0  4.54590  8.1674  -2.4586  -1.46210  0\n",
              "1  3.86600 -2.6383   1.9242   0.10645  0\n",
              "2  3.45660  9.5228  -4.0112  -3.59440  0\n",
              "3  0.32924 -4.4552   4.5718  -0.98880  0\n",
              "4  4.36840  9.6718  -3.9606  -3.16250  0"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    }
  ]
}